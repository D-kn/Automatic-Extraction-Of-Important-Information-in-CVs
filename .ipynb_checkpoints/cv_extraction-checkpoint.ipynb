{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45697,
     "status": "ok",
     "timestamp": 1615512918766,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "NRF6LU72KczQ",
    "outputId": "88bd8a74-ce1e-41c2-ed89-287047aa9591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197122,
     "status": "ok",
     "timestamp": 1615513119280,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "jk5yzJhqEpCD",
    "outputId": "44262578-b7c0-491d-f554-a807803b2e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/d4/81ce4f7ca0c563da98283501747af8248de5021aeaa4c69c9ae460e13efb/pdfminer3-2018.12.3.0.tar.gz (5.0MB)\n",
      "\u001b[K     |████████████████████████████████| 5.0MB 5.7MB/s \n",
      "\u001b[?25hCollecting pycryptodome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 42.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer3) (2.3.0)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer3) (3.0.4)\n",
      "Building wheels for collected packages: pdfminer3\n",
      "  Building wheel for pdfminer3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pdfminer3: filename=pdfminer3-2018.12.3.0-cp37-none-any.whl size=117805 sha256=5eeb3191200368fc33ba135d478e62d7a9bd85845e4d8c383ffe60ba3428d495\n",
      "  Stored in directory: /root/.cache/pip/wheels/27/35/61/7d4c0c350683751100186de869681db04d6f2c5ab603eb0510\n",
      "Successfully built pdfminer3\n",
      "Installing collected packages: pycryptodome, pdfminer3\n",
      "Successfully installed pdfminer3-2018.12.3.0 pycryptodome-3.10.1\n",
      "Collecting XlsxWriter\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/41/bf1aae04932d1eaffee1fc5f8b38ca47bbbf07d765129539bc4bcce1ce0c/XlsxWriter-1.3.7-py2.py3-none-any.whl (144kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 4.0MB/s \n",
      "\u001b[?25hInstalling collected packages: XlsxWriter\n",
      "Successfully installed XlsxWriter-1.3.7\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
      "Collecting xlutils\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/55/e22ac73dbb316cabb5db28bef6c87044a95914f713a6e81b593f8a0d2f79/xlutils-2.0.0-py2.py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: xlrd>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from xlutils) (1.1.0)\n",
      "Requirement already satisfied: xlwt>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from xlutils) (1.3.0)\n",
      "Installing collected packages: xlutils\n",
      "Successfully installed xlutils-2.0.0\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.0.1)\n",
      "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
      "Collecting docx2txt\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-cp37-none-any.whl size=3963 sha256=e943ebe03b8890e488557a21a1dd232813fa2bab2b7b316b6296218c30ca2a18\n",
      "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
      "Collecting en_core_web_lg==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
      "\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (54.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
      "Building wheels for collected packages: en-core-web-lg\n",
      "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180944 sha256=ec0a7c6d70cc7eb95b67f982bc4a5af89ed83d3b00dbbd06df8a97eaf78dcd41\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fua5edvq/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
      "Successfully built en-core-web-lg\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfminer3\n",
    "! pip install XlsxWriter\n",
    "! pip install xlrd\n",
    "! pip install xlutils\n",
    "! pip install openpyxl\n",
    "! pip install docx2txt\n",
    "! pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5678,
     "status": "ok",
     "timestamp": 1615513621149,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "glSHyf0-Mz3e",
    "outputId": "89acea0d-1097-415e-d238-1f25b13ffee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.layout import LAParams, LTTextBox, LTTextLine\n",
    "import nltk, os, subprocess, code, glob, re, traceback, sys, inspect\n",
    "from pdfminer3.converter import TextConverter\n",
    "from pdfminer3.pdfparser import PDFParser\n",
    "from pdfminer3.pdfdocument import PDFDocument\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pprint import pprint\n",
    "from xlwt import Workbook\n",
    "import fnmatch\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import xlsxwriter\n",
    "import xlrd\n",
    "import spacy\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "from xlrd import open_workbook\n",
    "from xlutils.copy import copy\n",
    "from openpyxl import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 18736,
     "status": "ok",
     "timestamp": 1615513799089,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "oCwhseJMIuot",
    "outputId": "c6b58656-6864-48fa-f5d6-dfd35bdf75cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-d5fc8689-7bf4-44ee-bd53-c4d735753cdf\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-d5fc8689-7bf4-44ee-bd53-c4d735753cdf\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving constants.py to constants (1).py\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('constants.py','wb').write(src)\n",
    "import constants as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1302,
     "status": "ok",
     "timestamp": 1615513836094,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "3dp0zZXVLrCR"
   },
   "outputs": [],
   "source": [
    "cvsPath='/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_pdf/'\n",
    "cvsPathRed='/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_red/'\n",
    "cvsTextPath='/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/'\n",
    "excelPath = '/content/drive/MyDrive/RECONNAISSSANCE_FORME/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz4qglVGXg0S"
   },
   "source": [
    "##**Fonction pour convertir un pdf en texte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 1789,
     "status": "ok",
     "timestamp": 1615513836597,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "4V6Bd5EoAWK1"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "     with open(pdf_path, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClICoZS8Xqc1"
   },
   "source": [
    "##**Fonction pour convertir un doc en texte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1781,
     "status": "ok",
     "timestamp": 1615513836599,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "D-DVgHi-Aa_u"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_doc(doc_path):\n",
    "    txt = docx2txt.process(doc_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JpKrFD9X0mm"
   },
   "source": [
    "##**Fonction pour convertir pdf ou doc, docx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1615513836601,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "-4gDotLdAmpW"
   },
   "outputs": [],
   "source": [
    "def extract_text(file_path, extension):\n",
    "    text = ''\n",
    "    if extension == 'pdf':\n",
    "        for page in extract_text_from_pdf(file_path):\n",
    "            text += ' ' + page\n",
    "    elif extension == 'docx' or extension == 'doc':\n",
    "        text = extract_text_from_doc(file_path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn04Zg7eYIko"
   },
   "source": [
    "##**Fonction pour génerer un texte en txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1767,
     "status": "ok",
     "timestamp": 1615513836606,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "kLUBX__TQv19"
   },
   "outputs": [],
   "source": [
    "def text_to_txt_file(cvsTextePath,text):\n",
    "  if os.path.exists(cvsTextePath+'.txt'):\n",
    "    os.remove(cvsTextePath+'.txt')\n",
    "    f = open(cvsTextePath+'.txt', \"x\")\n",
    "    text_file = open(cvsTextePath+'.txt', \"w\")\n",
    "    n = text_file.write(text)\n",
    "    text_file.close()\n",
    "  else:\n",
    "    f = open(cvsTextePath+'.txt', \"x\")\n",
    "    text_file = open(cvsTextePath+'.txt', \"w\")\n",
    "    n = text_file.write(text)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1752,
     "status": "ok",
     "timestamp": 1615513836607,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "3RZE8j0pOCTI"
   },
   "outputs": [],
   "source": [
    "def generate_text_files(datasetFolder, textFolder=cvsTextPath):\n",
    "  files = os.listdir(datasetFolder)\n",
    "  print(\"Nombre de fichier : \" + str(len(files)))\n",
    "  # Create cvs text directory if don't exist\n",
    "  if not os.path.exists(textFolder):\n",
    "    os.mkdir(textFolder)\n",
    "    print(\"Répertoire \", textFolder, \" créer \")\n",
    "  else:\n",
    "    print(\"Répertoire existe déjà\")\n",
    "  for path, _, files in os.walk(datasetFolder):\n",
    "    for file in files:\n",
    "          fullname = os.path.join(datasetFolder, file)\n",
    "          file_name = os.path.basename(fullname)\n",
    "          x = file_name.split('.')\n",
    "          extension = x[1]\n",
    "          nom = x[0]\n",
    "          textPdf = extract_text(fullname,extension)\n",
    "          text_to_txt_file(cvsTextPath+nom,textPdf)\n",
    "  print('Nombre de fichier créer: '+str(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qzm0zFjhZFQy"
   },
   "source": [
    "##**Fonction pour extraire les adresses emails dans un document txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 1744,
     "status": "ok",
     "timestamp": 1615513836609,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "G8tmaNNCXZe-"
   },
   "outputs": [],
   "source": [
    "def extract_mail(document):\n",
    "  email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", document)\n",
    "  if email:\n",
    "    try:\n",
    "      return email[0].split()[0].strip(';')\n",
    "    except IndexError:\n",
    "      return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9lV4p-mZeHZ"
   },
   "source": [
    "##**Fonction pour extraire les numéros de téléphone dans un document txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1737,
     "status": "ok",
     "timestamp": 1615513836612,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "HM2_isggxLks"
   },
   "outputs": [],
   "source": [
    "def extract_telephone_number(document):\n",
    "    for number in re.findall(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', document):\n",
    "      return number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaLNNOuOZ7tw"
   },
   "source": [
    "##**Fonction pour extraire les noms et prenoms dans un document txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 14152,
     "status": "ok",
     "timestamp": 1615513849037,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "B71oIHGc5xy4"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "def extract_name(document, matcher = Matcher(nlp.vocab)):\n",
    "    pattern = [cs.NAME_PATTERN]\n",
    "    matcher.add('NAME', None, *pattern)\n",
    "    nlp_text = nlp(document)\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        if 'name' not in span.text.lower():\n",
    "            return span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_bPpEqIazSp"
   },
   "source": [
    "##**Fonction pour extraire les compétances dans un document txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 14144,
     "status": "ok",
     "timestamp": 1615513849039,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "q9_VCfVyGHFk"
   },
   "outputs": [],
   "source": [
    "def extract_skills(document):\n",
    "    doc = nlp(document)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    file_path = '/content/drive/MyDrive/RECONNAISSSANCE_FORME/skills.csv'\n",
    "    data = pd.read_csv(file_path) \n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    # check for bi-grams and tri-grams\n",
    "    for token in doc.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVvMLTd9dlEy"
   },
   "source": [
    "##**Fonction pour extraire les niveau d'études**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 14132,
     "status": "ok",
     "timestamp": 1615513849040,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "6UB8mMFJdkDD"
   },
   "outputs": [],
   "source": [
    "def extract_education(document):\n",
    "    #STOPWORDS = set(stopwords.words('english')) \n",
    "    nlp_text = nlp(document)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in cs.EDUCATIONS and tex not in cs.STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkNMWvH6qDZA"
   },
   "source": [
    "##**Fonction pour extraire les expéreinces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 14119,
     "status": "ok",
     "timestamp": 1615513849041,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "SGLi3C2ZqQcT"
   },
   "outputs": [],
   "source": [
    "def extract_experience1(resume_text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization\n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize\n",
    "    filtered_sentence = [\n",
    "            w for w in word_tokens if w not\n",
    "            in stop_words and wordnet_lemmatizer.lemmatize(w)\n",
    "            not in stop_words\n",
    "        ]\n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "\n",
    "    test = []\n",
    "\n",
    "    for vp in list(\n",
    "        cs.subtrees(filter=lambda x: x.label() == 'P')\n",
    "    ):\n",
    "        test.append(\" \".join([\n",
    "            i[0] for i in vp.leaves()\n",
    "            if len(vp.leaves()) >= 2]).lower()\n",
    "        )\n",
    "    #print(test)\n",
    "    # Search the word 'experience' in the chunk and\n",
    "    # then print out the text after it\n",
    "    for i, x in enumerate(test):\n",
    "      #print(x)\n",
    "      if (x and 'expériences' in x.lower()):\n",
    "        return [x[x.lower().index('expériences') +10:]]\n",
    "      elif (x and 'expérience' in x.lower()):\n",
    "        return [x[x.lower().index('expérience') +10:]]\n",
    "      elif (x and 'experience' in x.lower()):\n",
    "        return [x[x.lower().index('experience') +10:]]\n",
    "      elif (x and 'professionelle' in x.lower()):\n",
    "        return [x[x.lower().index('professionelle') +10:]]\n",
    "      elif (x and 'professionnelles' in x.lower()):\n",
    "        return [x[x.lower().index('professionnelles') +10:]]\n",
    "      elif (x and 'professionnelle' in x.lower()):\n",
    "        return [x[x.lower().index('professionnelle') +10:]]     \n",
    "        \n",
    "\n",
    "    '''x = [\n",
    "        x[x.lower().index('experience') + 10:] or x[x.lower().index('Expérience') + 10:]\n",
    "        for i, x in enumerate(test)\n",
    "        if ((x and 'experience' in x.lower()) or (x and 'Expérience' in x.lower()))\n",
    "    ]'''\n",
    "    #return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPXlvvlHcVx6"
   },
   "source": [
    "##**Fonction pour créer un fichier excel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 14113,
     "status": "ok",
     "timestamp": 1615513849043,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "KFwwvGMsISUe"
   },
   "outputs": [],
   "source": [
    "def create_excel_file(fileFolder=excelPath):\n",
    "  workbook = xlsxwriter.Workbook(fileFolder+'extraction.xlsx')\n",
    "  worksheet = workbook.add_worksheet()\n",
    "  # Add a bold format to use to highlight cells.\n",
    "  bold = workbook.add_format({'bold': True})\n",
    "  # Write some simple text.\n",
    "  worksheet.write(0, 0, 'FICHIER', bold)\n",
    "  worksheet.set_column(0, 0, 20)\n",
    "  worksheet.write(0, 1, 'IDENTITE', bold)\n",
    "  worksheet.set_column(0, 1, 50)\n",
    "  worksheet.write(0, 2,'EMAILS', bold)\n",
    "  worksheet.set_column(0, 2, 40)\n",
    "  worksheet.write(0, 3, 'CONTACTS', bold)\n",
    "  worksheet.set_column(0, 3, 40)\n",
    "  worksheet.write(0, 4, 'COMPETENCES', bold)\n",
    "  worksheet.set_column(0, 4, 100)\n",
    "  worksheet.write(0, 5, 'EDUCATIONS', bold)\n",
    "  worksheet.set_column(0, 5, 100)\n",
    "  worksheet.write(0, 6, 'EXPERIENCES', bold)\n",
    "  worksheet.set_column(0, 6, 100)\n",
    "  workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dme4aOqTS0On"
   },
   "source": [
    "##**Removing Ansi from text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 14107,
     "status": "ok",
     "timestamp": 1615513849043,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "0V2cFmpyMfdr"
   },
   "outputs": [],
   "source": [
    "def escape_ansi(line):\n",
    "    ansi_escape =re.compile(r'(\\x9B|\\x1B\\[)[0-?]*[ -\\/]*[@-~]')\n",
    "    return ansi_escape.sub('', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dYoL_VtTQBP"
   },
   "source": [
    "##**Fonction pour extraire les expéreinces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 14100,
     "status": "ok",
     "timestamp": 1615513849044,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "KsKPi7xVHNC3"
   },
   "outputs": [],
   "source": [
    "def extract_experience(data, e_val = 30, split_val = 10):\n",
    "  lines = [escape_ansi(line) for line in data.split('\\n')] \n",
    "  experience_indexes = list()\n",
    "  \n",
    "  PUBLICATIONS_indexes, indexes  = list(), list()\n",
    "  for index, line in enumerate(data.split('\\n')):\n",
    "    if len(line.split()) < split_val:\n",
    "            ln = line\n",
    "            splitted = [l[:-1] for l in line.lower().split() if len(ln.lower().split())>=1]\n",
    "            splitted = splitted[:3]\n",
    "            check = ln.lower().split()[:3]\n",
    "            if (set(check) & set(cs.RESUME_SECTIONS)) or (set(splitted) & set(cs.RESUME_SECTIONS)):\n",
    "                words = line.split()\n",
    "                indexes.append(index)\n",
    "                try:\n",
    "                    if ((words[0].lower() in cs.EXPERIENCE) or (words[1][:-1].lower() in cs.EXPERIENCE) or (words[1].lower() in cs.EXPERIENCE) or (words[2].lower() in EXPERIENCE) or (words[0][:-1].lower() in EXPERIENCE)  or (words[2][:-1].lower() in EXPERIENCE)):\n",
    "                        experience_indexes.append(index)\n",
    "                except:\n",
    "                    if (words[0].lower() in cs.EXPERIENCE) or (words[0][:-1].lower() in cs.EXPERIENCE):\n",
    "                        experience_indexes.append(index)\n",
    "\n",
    "  sections_dict = {\"experience\":''}\n",
    "  experience_string = ''\n",
    "  count = 0\n",
    "  try:\n",
    "    for exp in experience_indexes:\n",
    "      stop = [f for f in indexes if f>exp]\n",
    "      try:\n",
    "        end = stop[0]\n",
    "      except:\n",
    "        end = len(lines)\n",
    "      count = 0\n",
    "      for i in range(exp,end):\n",
    "        experience_string += lines[i] + '\\n' \n",
    "    for e in experience_string.split('\\n'):\n",
    "      if len(e.split()) < 10:\n",
    "        count += 1   \n",
    "    flag = False\n",
    "  except:\n",
    "    pass\n",
    "  if count < e_val:\n",
    "    experience_string = ''\n",
    "    _string = ''\n",
    "    index_list = []\n",
    "    for exp in experience_indexes:\n",
    "      try:\n",
    "        for i in range(40):\n",
    "          if (exp+i) not in index_list:\n",
    "            _string += lines[exp+i] + '\\n'\n",
    "            index_list.append(exp+i)\n",
    "      except:\n",
    "        try:\n",
    "          for i in range(20):\n",
    "            if (exp+i) not in index_list:\n",
    "              _string += lines[exp+i] + '\\n'\n",
    "              index_list.append(exp+i)\n",
    "        except:\n",
    "          try:\n",
    "            for i in range(10):\n",
    "              if (exp+i) not in index_list:\n",
    "                _string += lines[exp+i] + '\\n'\n",
    "                index_list.append(exp+i)\n",
    "          except:\n",
    "              try:\n",
    "                for i in range(5):\n",
    "                  if (exp+i) not in index_list:\n",
    "                    _string += lines[exp+i] + '\\n'\n",
    "                    index_list.append(exp+i)\n",
    "              except:\n",
    "                pass\n",
    "      experience_string += _string\n",
    "      _string = '' \n",
    "        \n",
    "  sections_dict['experience'] = experience_string\n",
    "  return sections_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1615520750005,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "PV0npBm0W0Ri"
   },
   "outputs": [],
   "source": [
    "def exctractFromCV():\n",
    "  #Generation du fichier txt à partir des cv en pdf\n",
    "  generate_text_files(cvsPathRed)\n",
    "  indexLigne = 2\n",
    "  create_excel_file()\n",
    "  for path, _, files in os.walk(cvsTextPath):\n",
    "    for file in files:\n",
    "      if fnmatch.fnmatch(file, '*.txt'):\n",
    "        fullname = os.path.join(cvsTextPath, file)\n",
    "        file_name = os.path.basename(fullname)\n",
    "        print(fullname)\n",
    "        f = open(fullname, \"r\")\n",
    "        cvTxt = f.read()\n",
    "        mails = extract_mail(cvTxt)\n",
    "        numeros = extract_telephone_number(cvTxt)\n",
    "        names = extract_name(cvTxt)\n",
    "        educations = extract_education(cvTxt)\n",
    "        skills = extract_skills(cvTxt)\n",
    "        experiences = extract_experience(cvTxt)\n",
    "        experiences = experiences.get('experience').replace('\\n', ' ').replace('\\n\\n',' ')\n",
    "        wb=load_workbook(excelPath+'extraction.xlsx')\n",
    "        ws=wb[\"Sheet1\"]\n",
    "        fileNameCells = ws.cell(indexLigne,1)\n",
    "        nameCells = ws.cell(indexLigne,2)\n",
    "        mailCells = ws.cell(indexLigne,3)\n",
    "        numeroCells = ws.cell(indexLigne,4)\n",
    "        skillsCells = ws.cell(indexLigne,5)\n",
    "        educationsCells = ws.cell(indexLigne,6)\n",
    "        experiencesCells = ws.cell(indexLigne,7)\n",
    "        fileNameCells.value=str(file_name.split('.')[0]).encode(\"ascii\",errors=\"ignore\")\n",
    "        nameCells.value=str(names).encode(\"ascii\",errors=\"ignore\")\n",
    "        numeroCells.value=str(numeros).encode(\"ascii\",errors=\"ignore\")\n",
    "        mailCells.value=str(mails).encode(\"ascii\",errors=\"ignore\")\n",
    "        skillsCells.value=str(skills).encode(\"ascii\",errors=\"ignore\")\n",
    "        educationsCells.value=str(educations).encode(\"ascii\",errors=\"ignore\")\n",
    "        experiencesCells.value=str(experiences.encode(\"ascii\",errors=\"ignore\")).replace('\\n', ' ').replace('\\n\\n',' ')\n",
    "        indexLigne = indexLigne+1\n",
    "        wb.save(excelPath+'extraction.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12282,
     "status": "ok",
     "timestamp": 1615520766072,
     "user": {
      "displayName": "KANKUE ALOGUIDI KUDJOH",
      "photoUrl": "",
      "userId": "10708124459872631573"
     },
     "user_tz": -420
    },
    "id": "BH9KPI4Afeus",
    "outputId": "a683c536-4562-41c1-d8a5-7eb5479f925b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichier : 11\n",
      "Répertoire existe déjà\n",
      "Nombre de fichier créer: 11\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/225.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/410.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/10_KUDJOH_CV.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/PAZIMNA_CV_M2__Juste.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/cv_mimi.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/CV_OUEDRAOGO5.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/CV_OUEDRAOGO4.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/CV_MAMBA KABALA DENIS.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/CV.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/38.txt\n",
      "/content/drive/MyDrive/RECONNAISSSANCE_FORME/CVs_text/35.txt\n"
     ]
    }
   ],
   "source": [
    "exctractFromCV()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNzGYKdJNKMPsFWDf0lsEck",
   "collapsed_sections": [],
   "name": "cv_extraction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
